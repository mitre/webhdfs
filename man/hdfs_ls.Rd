% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/ls.R
\name{hdfs_ls}
\alias{clean_liststatus_columns}
\alias{hdfs_ls}
\title{List a directory}
\usage{
hdfs_ls(path, recursive = FALSE, concise = FALSE, user = get_user(),
  return_type = get_return_type())

clean_liststatus_columns(dat)
}
\arguments{
\item{path}{Character containing file system path}

\item{recursive}{Logical indicator of whether to resursively list individual
files within sub-directories.  Default FALSE.}

\item{concise}{Logical indicator of whether to return only a select subset of
columns.  Default FALSE.}

\item{user}{Character username to use in WebHDFS operation.  If not provided,
\code{webhdfs.user} will be used and if that has not been set, a call to
\code{\link{guess_user}} will be made.}

\item{return_type}{character string. See \code{\link{set_return_type}} for
details and options.}

\item{dat}{Data frame containing the result of a LISTSTATUS command}
}
\value{
\code{data.frame} (or other requested type) containing output from
  the WebHDFS operation
}
\description{
Shortcut for WebHDFS LISTSTATUS operation
}
\details{
Wildcards (*) in the provided \code{path} are handled by getting all contents
at that level of the directory structure, and appending any remaining
portions of the \code{path} below that level.

When \code{concise = TRUE}, function \code{clean_liststatus_columns} is
called to limit the set of columns from the WebHDFS \code{LISTSTATUS}
operation to the following fields in this order: \code{pathSuffix},
\code{childrenNum}, \code{length}, \code{group}, \code{modificationTime},
\code{owner}, \code{permission}, \code{type}.  Note that this places the
\code{pathSuffix} field as the first column, which differs from the default
ordering. When \code{concise = FALSE}, the results include the full set of
columns returned by \code{LISTSTATUS}, in the original order.
}
\examples{
\dontrun{
# basic usage to list directory contents
hdfs_ls("/data/")

# recursively list files within sub-directories
hdfs_ls("/data/blah/2016", recursive = TRUE)

# use wildcard to get the first day of every month in 2016
hdfs_ls("/data/blah/2016/*/01")

# use wildcards to get all days in July in every year
hdfs_ls("/data/blah/*/07/*")
}


}

